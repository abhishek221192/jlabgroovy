#summary Using Native BLAS for fast computations using JBLAS.

= Introduction =

_`JBLAS`_ `is the fastest Java library (as far as I can know) for linear algebra. To obtain speed it utilizes native BLAS routines. The project page is: http://mikiobraun.github.io/jblas/`

`GroovyLab automatically installs the relevant staff, therefore it is easy to perform fast computations using JBLAS. We give some examples. `

== Matrix multiplication ==

{{{

// Demonstrate the difference in performance beteen Native BLAS and Java
// for matrix multiplication

 import  org.jblas.*

 n = 1000
 
 x = DoubleMatrix.randn(n, n)
 y = DoubleMatrix.randn(n, n)
 z = DoubleMatrix.randn(n, n)        

 println("Multiplying DOUBLE matrices of size "+ n)

 tic()
 SimpleBlas.gemm(1.0, x, y, 0.0, z)
 tm = toc()

// test with Java multiplication
 xm = rand(n)
 tic()
 xmxm =xm*xm
 tmJ = toc()
        
 println("Time Native = "+tm+", time Java = "+tmJ)

}}}

== `Switching of the GroovySci Matrix class to use JBLAS` ==

`We can explore the metaprogramming facilities of the Groovy language, in order to dynamically bind the code of many important operations of the` *`Matrix`* `class, with the JBLAS implementations.`

`Therefore, using native implementations offered by JBLAS, we can obtain significant speedup relative to the Java implementations (about 4 to 8 times speedup). `


`For example, here is how we can reprogram the ` _`matrix multiplication`_ `routine of the ` _`Matrix`_ `class, in order to use JBLAS .`

{{{
// reimplement Matrix-Matrix  multiplication using JBLAS
groovySci.math.array.Matrix.metaClass.multiply = { 
   groovySci.math.array.Matrix m ->   // the input Matrix

 // transform the input matrix to the JBLAS representation
     dm =  new org.jblas.DoubleMatrix(m.toDoubleArray())
 // transform the receiver to the JBLAS representation
     dmthis = new org.jblas.DoubleMatrix(delegate.toDoubleArray())
 // fast multiply using JBLAS Native BLAS
     mulRes = dmthis.mmul(dm)

 // return back result as a double [][] array
    groovySci.math.array.JBLASUtils.JBLASDoubleMatrixToDouble2D(mulRes)
}

}}}

`After executing the former script, we can test the speedup. Here, we note that JBLAS obtains significant speed for Linux and Win32 platforms (and MacOS I suppose but I can't test) but not for Win64, where the speed is similar to the Java implementation. Here is a test code: `

{{{

x = rand(2000, 2000) // a large 2000X2000 matrix  

tic()
y = x*x  // multiply with JBLAS Native BLAS
tmJBLAS = toc()

xx=Rand(2000, 2000)  // a large 2000X2000 double[][] array
tic()
yy=xx*xx  // multiply with Java
tmJ = toc()

println("time for matrix multiplication using Native BLAS = "+tmJBLAS+", time with Java = "+tmJ)
}}}

`The results for my Linux based PC is: `
{{{
time for matrix multiplication using Native BLAS = 1.956, time with Java = 10.575
}}}

== Eigendecomposition with JBLAS ==

`Similarly we can utilize JBLAS for more complex tasks as eigendecomposition:`

{{{

// compute the eigenvalues of a general matrix using JBLAS
groovySci.math.array.Matrix.metaClass.jblasEigenValues = {
	// transform the receiver to the JBLAS representation
	dmthis = new org.jblas.DoubleMatrix(delegate.toDoubleArray())
	org.jblas.Eigen.eigenvalues(dmthis)
 }

}}}

`To test our routine: `
{{{
xs = rand(4,4)
xeigs = xs.jblasEigenValues()
}}}

`Similarly for eigenvectors: `

{{{

// compute the eigenvectors of a general matrix using JBLAS
//   returns an array of ComplexDoubleMatrix objects containing the eigenvectors
//          stored as the columns of the first matrix, and the eigenvalues as the
//         diagonal elements of the second matrix.
groovySci.math.array.Matrix.metaClass.jblasEigenVectors = {
	// transform the receiver to the JBLAS representation
	dmthis = new org.jblas.DoubleMatrix(delegate.toDoubleArray())
	org.jblas.Eigen.eigenvectors(dmthis)
}

}}}

`And, we can test: `
{{{
xs2 = rand(3,3)
xeigvecs = xs2.jblasEigenVectors()

}}}

`Similarly for symmetric matrices. `

{{{

     
//  Compute the eigenvalues for a symmetric matrix.
groovySci.math.array.Matrix.metaClass.jblas_symmetricEigenvalues = {
	// transform the receiver to the JBLAS representation
	dmthis = new org.jblas.DoubleMatrix(delegate.toDoubleArray())
	org.jblas.Eigen.symmetricEigenvalues(dmthis)
}

//  Computes the eigenvalues and eigenvectors for a symmetric matrix.
//  returns an array of DoubleMatrix objects containing the eigenvectors
//         stored as the columns of the first matrix, and the eigenvalues as
//         diagonal elements of the second matrix.
groovySci.math.array.Matrix.metaClass.jblas_symmetricEigenvectors = {
	// transform the receiver to the JBLAS representation
	dmthis = new org.jblas.DoubleMatrix(delegate.toDoubleArray())
	org.jblas.Eigen.symmetricEigenvectors(dmthis)
}

}}}

`JBLAS also offers a routine for fast Cholesky Decomposition, which can be wrapped as: `
{{{

//  Compute Cholesky decomposition of A
//      @param A symmetric, positive definite matrix (only upper half is used)
//      @return upper triangular matrix U such that  A = U' * U
 groovySci.math.array.Matrix.metaClass.jblas_cholesky = {
 	// transform the receiver to the JBLAS representation
	dmthis = new org.jblas.DoubleMatrix(delegate.toDoubleArray())
	org.jblas.Decompose.cholesky(dmthis)    
 }
 }}}	