#summary Using Native BLAS for fast computations using JBLAS.

= Introduction =

_`JBLAS`_ `is the fastest Java library (as far as I can know) for linear algebra. To obtain speed it utilizes native BLAS routines. The project page is: http://mikiobraun.github.io/jblas/`

`GroovyLab automatically installs the relevant staff, therefore it is easy to perform fast computations using JBLAS. We give some examples. `

== Matrix multiplication ==

{{{

// Demonstrate the difference in performance beteen Native BLAS and Java
// for matrix multiplication

 import  org.jblas.*

 n = 1000
 
 x = DoubleMatrix.randn(n, n)
 y = DoubleMatrix.randn(n, n)
 z = DoubleMatrix.randn(n, n)        

 println("Multiplying DOUBLE matrices of size "+ n)

 tic()
 SimpleBlas.gemm(1.0, x, y, 0.0, z)
 tm = toc()

// test with Java multiplication
 xm = rand(n)
 tic()
 xmxm =xm*xm
 tmJ = toc()
        
 println("Time Native = "+tm+", time Java = "+tmJ)

}}}

== `Switching of the GroovySci Matrix class to use JBLAS` ==

`We can explore the metaprogramming facilities of the Groovy language, in order to dynamically bind the code of many important operations of the` *`Matrix`* `class, with the JBLAS implementations.`

`Therefore, using native implementations offered by JBLAS, we can obtain significant speedup relative to the Java implementations (about 4 to 8 times speedup). `


`For example, here is how we can reprogram the ` _`matrix multiplication`_ `routine of the ` _`Matrix`_ `class, in order to use JBLAS .`

 