#summary Using Native BLAS for fast computations using JBLAS.

= Introduction =

_`JBLAS`_ `is the fastest Java library (as far as I can know) for linear algebra. To obtain speed it utilizes native BLAS routines. The project page is: http://mikiobraun.github.io/jblas/`

`GroovyLab automatically installs the relevant staff, therefore it is easy to perform fast computations using JBLAS. We give some examples. `

== Matrix multiplication ==

{{{

// Demonstrate the difference in performance beteen Native BLAS and Java
// for matrix multiplication

 import  org.jblas.*

 n = 1000
 
 x = DoubleMatrix.randn(n, n)
 y = DoubleMatrix.randn(n, n)
 z = DoubleMatrix.randn(n, n)        

 println("Multiplying DOUBLE matrices of size "+ n)

 tic()
 SimpleBlas.gemm(1.0, x, y, 0.0, z)
 tm = toc()

// test with Java multiplication
 xm = rand(n)
 tic()
 xmxm =xm*xm
 tmJ = toc()
        
 println("Time Native = "+tm+", time Java = "+tmJ)

}}}

== `Switching of the GroovySci Matrix class to use JBLAS` ==

`We can explore the metaprogramming facilities of the Groovy language, in order to dynamically bind the code of many important operations of the` *`Matrix`* `class, with the JBLAS implementations.`

`Therefore, using native implementations offered by JBLAS, we can obtain significant speedup relative to the Java implementations (about 4 to 8 times speedup). `


`For example, here is how we can reprogram the ` _`matrix multiplication`_ `routine of the ` _`Matrix`_ `class, in order to use JBLAS .`

{{{
// reimplement Matrix-Matrix  multiplication using JBLAS
groovySci.math.array.Matrix.metaClass.multiply = { 
   groovySci.math.array.Matrix m ->   // the input Matrix

 // transform the input matrix to the JBLAS representation
     dm =  new org.jblas.DoubleMatrix(m.toDoubleArray())
 // transform the receiver to the JBLAS representation
     dmthis = new org.jblas.DoubleMatrix(delegate.toDoubleArray())
 // fast multiply using JBLAS Native BLAS
     mulRes = dmthis.mmul(dm)

 // return back result as a double [][] array
    groovySci.math.array.JBLASUtils.JBLASDoubleMatrixToDouble2D(mulRes)
}

}}}

`After executing the former script, we can test the speedup. Here, we note that JBLAS obtains significant speed for Linux and Win32 platforms (and MacOS I suppose but I can't test) but not for Win64, where the speed is similar to the Java implementation. Here is a test code: `

{{{

x = rand(2000, 2000) // a large 2000X2000 matrix  

tic()
y = x*x  // multiply with JBLAS Native BLAS
tmJBLAS = toc()

xx=Rand(2000, 2000)  // a large 2000X2000 double[][] array
tic()
yy=xx*xx  // multiply with Java
tmJ = toc()

println("time for matrix multiplication using Native BLAS = "+tmJBLAS+", time with Java = "+tmJ)
}}}

`The results for my Linux based PC is: `
{{{
time for matrix multiplication using Native BLAS = 1.956, time with Java = 10.575
}}}